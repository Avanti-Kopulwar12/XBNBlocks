# -*- coding: utf-8 -*-
"""hyp_2 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O1V-rN2LoE-zlXw26F3c9QS2Bb4inEOI
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.backends.cudnn as cudnn
import torchvision
import torchvision.transforms as transforms
import os
import time
import tarfile
import numpy as np
import matplotlib.pyplot as plt
from torchvision.datasets.utils import download_url
import pandas as pd

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {DEVICE}")

torch.manual_seed(42)
if DEVICE == 'cuda':
    torch.cuda.manual_seed(42)
    cudnn.benchmark = True

class StandardBlock(nn.Module):

    expansion = 4
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(StandardBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        if self.downsample is not None: residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out

class XBNBlock(nn.Module):

    expansion = 4
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(XBNBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)

        self.bn2 = nn.GroupNorm(num_groups=32, num_channels=planes)

        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        if self.downsample is not None: residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out

class ResNetLocation(nn.Module):
    def __init__(self, layers, num_classes=10, location_mode='uniform'):
        super(ResNetLocation, self).__init__()
        self.inplanes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # --- HYPOTHESIS 2 ---
        if location_mode == 'uniform':
            BlockType_Early = XBNBlock
        else:
            BlockType_Early = StandardBlock

        BlockType_Deep = XBNBlock

        self.layer1 = self._make_layer(BlockType_Early, 64, layers[0])
        self.layer2 = self._make_layer(BlockType_Early, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(BlockType_Early, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(BlockType_Deep, 512, layers[3], stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * 4, num_classes)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * 4:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * 4, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * 4),
            )
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * 4
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

def verify_architecture():
    print("--- VERIFYING ARCHITECTURES ---")

    model_uni = ResNetLocation([3, 4, 6, 3], location_mode='uniform')
    print(f"Uniform Mode | Layer 1 Type: {model_uni.layer1[0].__class__.__name__}")
    print(f"Uniform Mode | Layer 4 Type: {model_uni.layer4[0].__class__.__name__}")

    model_late = ResNetLocation([3, 4, 6, 3], location_mode='late_stage')
    print(f"Late-Stage   | Layer 1 Type: {model_late.layer1[0].__class__.__name__}")
    print(f"Late-Stage   | Layer 4 Type: {model_late.layer4[0].__class__.__name__}")

    print("\nEXPECTATION:")
    print("Uniform:    Layer 1 = XBNBlock, Layer 4 = XBNBlock")
    print("Late-Stage: Layer 1 = StandardBlock, Layer 4 = XBNBlock")

verify_architecture()

def get_imagenette_loaders(batch_size):
    data_dir = './data/imagenette'
    if not os.path.exists(data_dir):
        print("Downloading ImageNette...")
        url = "https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz"
        download_url(url, root='./data', filename="imagenette.tgz", md5=None)
        with tarfile.open('./data/imagenette.tgz', 'r:gz') as tar:
            tar.extractall(path='./data')
        os.rename('./data/imagenette2-160', data_dir)

    stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
    train_tfms = transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(*stats)
    ])
    valid_tfms = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(*stats)
    ])

    trainset = torchvision.datasets.ImageFolder(root=f'{data_dir}/train', transform=train_tfms)
    validset = torchvision.datasets.ImageFolder(root=f'{data_dir}/val', transform=valid_tfms)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)
    validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size*2, shuffle=False, num_workers=2)
    return trainloader, validloader

def run_experiment(location_mode, epochs=10):
    print(f"\n{'='*40}")
    print(f"TRAINING MODEL: {location_mode} Placement")
    print(f"{'='*40}")

    BATCH_SIZE = 32
    LR = 0.001

    trainloader, validloader = get_imagenette_loaders(BATCH_SIZE)

    model = ResNetLocation([3, 4, 6, 3], num_classes=10, location_mode=location_mode)
    model = model.to(DEVICE)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=1e-4)

    start_time = time.time()
    best_acc = 0.0

    for epoch in range(epochs):
        model.train()
        correct = 0; total = 0

        for inputs, labels in trainloader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_acc = 100 * correct / total

        model.eval()
        val_correct = 0; val_total = 0
        with torch.no_grad():
            for inputs, labels in validloader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()

        val_acc = 100 * val_correct / val_total
        best_acc = max(best_acc, val_acc)
        print(f"Epoch {epoch+1}/{epochs} | Train: {train_acc:.2f}% | Val: {val_acc:.2f}%")

    total_time = time.time() - start_time
    print(f"--> DONE. Best Val: {best_acc:.2f}% | Time: {total_time:.1f}s")
    return best_acc, total_time

EPOCHS = 10
print("HYPOTHESIS 2: OPTIMIZING LOCATION")
acc_uni, time_uni = run_experiment('uniform', epochs=EPOCHS)

acc_late, time_late = run_experiment('late_stage', epochs=EPOCHS)

results_h2 = {
    'Uniform (Baseline)': acc_uni,
    'Late-Stage (Hypothesis)': acc_late
}

print("\nFINAL ACCURACY REPORT:")
for k, v in results_h2.items():
    print(f"{k}: {v:.2f}%")

print("\nFINAL SPEED REPORT (Total Training Time):")
print(f"Uniform:    {time_uni:.1f}s")
print(f"Late-Stage: {time_late:.1f}s")

names = list(results_h2.keys())
values = list(results_h2.values())

plt.figure(figsize=(8, 5))
colors = ['#1f77b4', '#ff7f0e']
bars = plt.bar(names, values, color=colors)

plt.ylim(min(values)-5, max(values)+2)
plt.ylabel('Validation Accuracy (%)')
plt.title('Hypothesis 2: Firewall Location Efficacy')
plt.grid(axis='y', linestyle='--', alpha=0.7)

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.1, f'{yval:.2f}%', ha='center', va='bottom', fontweight='bold')

plt.show()

def create_h2_summary_table(acc_uni, acc_late, time_uni, time_late):
    acc_diff = acc_late - acc_uni
    time_saved = time_uni - time_late
    speedup_pct = (time_saved / time_uni) * 100 if time_uni > 0 else 0

    data = {
        "Metric": ["Top-1 Accuracy", "Total Training Time", "Relative Performance"],
        "Uniform (Baseline)": [
            f"{acc_uni:.2f}%",
            f"{time_uni:.1f}s",
            "Baseline (1.0x)"
        ],
        "Late-Stage (Hypothesis)": [
            f"{acc_late:.2f}%",
            f"{time_late:.1f}s",
            f"{speedup_pct:.1f}% Faster" if time_saved > 0 else f"{abs(speedup_pct):.1f}% Slower"
        ],
        "Net Impact": [
            f"{acc_diff:.2f}%",
            f"{'-' if time_saved > 0 else '+'}{abs(time_saved):.1f}s",
            "Trade-off Result"
        ]
    }

    df = pd.DataFrame(data)
    return df

df_summary = create_h2_summary_table(acc_uni, acc_late, time_uni, time_late)

print("\n=== HYPOTHESIS 2: SUMMARY TABLE ===")
print(df_summary.to_markdown(index=False))

print("\n=== CONCLUSION ===")
if acc_late >= (acc_uni - 1.0):
    print("RESULT: Hypothesis supported.")
    print("Late-Stage placement preserves enough accuracy while changing speed.")
    print("We successfully avoided the computational cost of early blocking.")
else:
    print("RESULT: Hypothesis rejected.")
    print("Significant accuracy drop detected (>1%).")
    print("Conclusion: The 'Firewall' must be placed early (Uniform) to stop error propagation.")